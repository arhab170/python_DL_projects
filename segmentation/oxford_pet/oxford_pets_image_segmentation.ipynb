{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMzSqYJDhwFZ"
      },
      "source": [
        "# Image segmentation with a U-Net-like architecture\n",
        "\n",
        "**Description:** Image segmentation model trained from scratch on the Oxford Pets dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKD7AuthwFa"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h58Txpz5hwFb"
      },
      "outputs": [],
      "source": [
        "# !!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "# !!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "# !\n",
        "\n",
        "# !curl -O https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n",
        "# !curl -O https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n",
        "# !\n",
        "# !tar -xf images.tar.gz\n",
        "# !tar -xf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYcRXlqlhwFb"
      },
      "source": [
        "## Prepare paths of input images and target segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzP5q61MhwFc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "img_size = (160, 160)\n",
        "num_classes = 3\n",
        "batch_size = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from struct import unpack\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "marker_mapping = {\n",
        "    0xffd8: \"Start of Image\",\n",
        "    0xffe0: \"Application Default Header\",\n",
        "    0xffdb: \"Quantization Table\",\n",
        "    0xffc0: \"Start of Frame\",\n",
        "    0xffc4: \"Define Huffman Table\",\n",
        "    0xffda: \"Start of Scan\",\n",
        "    0xffd9: \"End of Image\"\n",
        "}\n",
        "\n",
        "class JPEG:\n",
        "    def __init__(self, image_file):\n",
        "        with open(image_file, 'rb') as f:\n",
        "            self.img_data = f.read()\n",
        "    \n",
        "    def decode(self):\n",
        "        data = self.img_data\n",
        "        while(True):\n",
        "            marker, = unpack(\">H\", data[0:2])\n",
        "            # print(marker_mapping.get(marker))\n",
        "            if marker == 0xffd8:\n",
        "                data = data[2:]\n",
        "            elif marker == 0xffd9:\n",
        "                return\n",
        "            elif marker == 0xffda:\n",
        "                data = data[-2:]\n",
        "            else:\n",
        "                lenchunk, = unpack(\">H\", data[2:4])\n",
        "                data = data[2+lenchunk:]            \n",
        "            if len(data) == 0:\n",
        "                break        \n",
        "\n",
        "# Define image directory\n",
        "image_directory = \"./images\"\n",
        "\n",
        "# Get the list of image files\n",
        "image_files = os.listdir(image_directory)\n",
        "\n",
        "# List to store bad files\n",
        "bads = []\n",
        "\n",
        "# Iterate through images\n",
        "for img in tqdm(image_files):\n",
        "    image_path = os.path.join(image_directory, img)\n",
        "    image = JPEG(image_path) \n",
        "    try:\n",
        "        image.decode()   \n",
        "    except:\n",
        "        bads.append(img)\n",
        "\n",
        "# Remove bad files\n",
        "for name in bads:\n",
        "    os.remove(os.path.join(image_directory, name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "image_directory = \"./images\"\n",
        "annotation_directory = \"./annotations/trimaps\"\n",
        "\n",
        "# Get the list of image files\n",
        "image_files = os.listdir(image_directory)\n",
        "\n",
        "# Get the list of annotation files\n",
        "annotation_files = os.listdir(annotation_directory)\n",
        "\n",
        "# Iterate through annotation files\n",
        "for annotation_file in annotation_files:\n",
        "    # Check if the file is a PNG file and does not start with \".\"\n",
        "    if annotation_file.endswith('.png') and not annotation_file.startswith('.'):\n",
        "        # Extract the corresponding image file name\n",
        "        image_file_name = annotation_file.replace('.png', '.jpg')\n",
        "        # Check if the corresponding image file exists\n",
        "        if image_file_name not in image_files:\n",
        "            # If the corresponding image file doesn't exist, delete the annotation file\n",
        "            annotation_path = os.path.join(annotation_directory, annotation_file)\n",
        "            os.remove(annotation_path)\n",
        "            print(f\"Deleted {annotation_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Number of image samples:\", len(input_img_paths))\n",
        "print(\"Number of Target samples:\", len(target_img_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMXSy1FkhwFc"
      },
      "source": [
        "## What does one input image and corresponding segmentation mask look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjNQo2_LhwFd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from keras.utils import load_img\n",
        "from PIL import ImageOps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number = 3505\n",
        "# Display input image #7\n",
        "display(Image(filename=input_img_paths[number]))\n",
        "\n",
        "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "img = ImageOps.autocontrast(load_img(target_img_paths[number]))\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMTmKp2RhwFd"
      },
      "source": [
        "## Prepare dataset to load & vectorize batches of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn_dyQ-AhwFd"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow import data as tf_data\n",
        "from tensorflow import image as tf_image\n",
        "from tensorflow import io as tf_io\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_img = tf_io.read_file(train_input_img_paths[3])\n",
        "type(input_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_img = tf_io.decode_png(input_img, channels=3)\n",
        "input_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_img = tf_image.resize(input_img, img_size)\n",
        "input_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
        "input_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_img_masks(input_img_path, target_img_path):\n",
        "    input_img = tf_io.read_file(input_img_path)\n",
        "    input_img = tf_io.decode_png(input_img, channels=3)\n",
        "    input_img = tf_image.resize(input_img, img_size)\n",
        "    input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
        "\n",
        "    target_img = tf_io.read_file(target_img_path)\n",
        "    target_img = tf_io.decode_png(target_img, channels=1)\n",
        "    target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
        "    target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
        "\n",
        "    # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "    target_img -= 1\n",
        "    return input_img, target_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_dataset(\n",
        "    batch_size,\n",
        "    img_size,\n",
        "    input_img_paths,\n",
        "    target_img_paths,\n",
        "    max_dataset_len=None,\n",
        "):\n",
        "    \"\"\"Returns a TF Dataset.\"\"\"\n",
        "\n",
        "    def load_img_masks(input_img_path, target_img_path):\n",
        "        input_img = tf_io.read_file(input_img_path)\n",
        "        input_img = tf_io.decode_png(input_img, channels=3)\n",
        "        input_img = tf_image.resize(input_img, img_size)\n",
        "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
        "\n",
        "        target_img = tf_io.read_file(target_img_path)\n",
        "        target_img = tf_io.decode_png(target_img, channels=1)\n",
        "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
        "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
        "\n",
        "        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "        target_img -= 1\n",
        "        return input_img, target_img\n",
        "\n",
        "    # For faster debugging, limit the size of data\n",
        "    if max_dataset_len:\n",
        "        input_img_paths = input_img_paths[:max_dataset_len]\n",
        "        target_img_paths = target_img_paths[:max_dataset_len]\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
        "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNl1UxXIhwFe"
      },
      "source": [
        "## Prepare U-Net Xception-style model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hli-CEKhhwFe"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZeGegnOhwFf"
      },
      "source": [
        "## Set aside a validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNjzng1LhwFf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 1000\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate dataset for each split\n",
        "# Limit input files in `max_dataset_len` for faster epoch training time.\n",
        "# Remove the `max_dataset_len` arg when running with full dataset.\n",
        "train_dataset = get_dataset(\n",
        "    batch_size,\n",
        "    img_size,\n",
        "    train_input_img_paths,\n",
        "    train_target_img_paths,\n",
        "    max_dataset_len=1000,\n",
        ")\n",
        "valid_dataset = get_dataset(\n",
        "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlQSfjS7hwFf"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zjHyNj9hwFf"
      },
      "outputs": [],
      "source": [
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-4), loss=\"sparse_categorical_crossentropy\"\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "epochs = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=callbacks\n",
        "    # verbose=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBwpDSohhwFg"
      },
      "source": [
        "## Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsFwjAPbhwFg"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for all images in the validation set\n",
        "\n",
        "val_dataset = get_dataset(\n",
        "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
        ")\n",
        "val_preds = model.predict(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_mask(i, width=300, height=200):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    mask = np.argmax(val_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
        "    display(img.resize((width, height)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Display results for validation image #10\n",
        "i =26\n",
        "width = 400\n",
        "height = 300\n",
        "\n",
        "# Display input image\n",
        "display(Image(filename=val_input_img_paths[i], width=width, height=height))\n",
        "\n",
        "# Display ground-truth target mask\n",
        "img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
        "display(img.resize((width, height)))\n",
        "\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i, width=width, height=height)  # Note that the model only sees inputs at 150x150."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "oxford_pets_image_segmentation",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
